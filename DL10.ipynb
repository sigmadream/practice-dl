{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: streamlit\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "st.title('Uber pickups in NYC')\n",
        "\n",
        "DATE_COLUMN = 'date/time'\n",
        "DATA_URL = ('https://s3-us-west-2.amazonaws.com/'\n",
        "            'streamlit-demo-data/uber-raw-data-sep14.csv.gz')\n",
        "\n",
        "@st.cache_data\n",
        "def load_data(nrows):\n",
        "    data = pd.read_csv(DATA_URL, nrows=nrows)\n",
        "    lowercase = lambda x: str(x).lower()\n",
        "    data.rename(lowercase, axis='columns', inplace=True)\n",
        "    data[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])\n",
        "    return data\n",
        "\n",
        "data_load_state = st.text('Loading data...')\n",
        "data = load_data(10000)\n",
        "data_load_state.text(\"Done! (using st.cache_data)\")\n",
        "\n",
        "if st.checkbox('Show raw data'):\n",
        "    st.subheader('Raw data')\n",
        "    st.write(data)\n",
        "\n",
        "st.subheader('Number of pickups by hour')\n",
        "hist_values = np.histogram(data[DATE_COLUMN].dt.hour, bins=24, range=(0,24))[0]\n",
        "st.bar_chart(hist_values)\n",
        "\n",
        "# Some number in the range 0-23\n",
        "hour_to_filter = st.slider('hour', 0, 23, 17)\n",
        "filtered_data = data[data[DATE_COLUMN].dt.hour == hour_to_filter]\n",
        "\n",
        "st.subheader('Map of all pickups at %s:00' % hour_to_filter)\n",
        "st.map(filtered_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ok, Let's LeNet-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install streamlit streamlit-drawable-canvas opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './LeNet5_20240129.pth'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m   model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./LeNet5_20240129.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m---> 58\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m st\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124m         # MNIST를 사용한 숫자 인식 예측 앱\u001b[39m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m st\u001b[38;5;241m.\u001b[39mmarkdown(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124m            ## 숫자를 그려보세요!\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[1], line 55\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m():\n\u001b[0;32m     54\u001b[0m   model \u001b[38;5;241m=\u001b[39m LeNet5()\n\u001b[1;32m---> 55\u001b[0m   model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./LeNet5_20240129.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "File \u001b[1;32mc:\\Users\\sigma\\Practices\\practice-dl\\.venv\\Lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[1;32mc:\\Users\\sigma\\Practices\\practice-dl\\.venv\\Lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[1;32mc:\\Users\\sigma\\Practices\\practice-dl\\.venv\\Lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './LeNet5_20240129.pth'"
          ]
        }
      ],
      "source": [
        "from streamlit_drawable_canvas import st_canvas\n",
        "import streamlit as st\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0)\n",
        "        self.act3 = nn.Tanh()\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(1*1*120, 84)\n",
        "        self.act4 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.act2(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.act3(self.conv3(x))\n",
        "        x = self.act4(self.fc1(self.flat(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def import_and_predict(img, model):\n",
        "  img_transform = transforms.Compose([transforms.Grayscale(), transforms.RandomInvert(p=1)])\n",
        "  img_new = img_transform(img)\n",
        "  composed = transforms.Compose([transforms.Resize(28), transforms.ToTensor()])\n",
        "  img_t = composed(img_new)\n",
        "  img_t = img_t.type(torch.float32)\n",
        "  x = img_t.expand(1, 1, 28, 28)\n",
        "  z = model(x)\n",
        "  z = nn.Softmax(dim=1)(z)\n",
        "  p_max, yhat = torch.max(z.data, 1)\n",
        "  p = float(format(p_max.numpy()[0], '.4f'))*100\n",
        "  yhat = int(float(yhat.numpy()[0]))\n",
        "  st.success(f\"작성하신 숫자는 {p:.2f} %로 {yhat}로 예측됩니다.\")\n",
        "\n",
        "def load_model():\n",
        "  model = LeNet5()\n",
        "  model.load_state_dict(torch.load('./LeNet5_20240129.pth'))\n",
        "  return model\n",
        "\n",
        "model = load_model()\n",
        "st.write(\"\"\"\n",
        "         # MNIST를 사용한 숫자 인식 예측 앱\n",
        "        \"\"\")\n",
        "st.markdown(\"\"\"\n",
        "            ## 숫자를 그려보세요!\n",
        "            \"\"\")\n",
        "st.write(\"Note: 숫자가 캔버스의 대부분을 차지하고 캔버스 중앙에 위치하도록 이미지를 그립니다.\")\n",
        "st.sidebar.header(\"사용자 입력\")\n",
        "\n",
        "# Specify brush parameters and drawing mode\n",
        "b_width = st.sidebar.slider(\"브러시 너비 선택: \", 1, 100, 10)\n",
        "b_color = st.sidebar.color_picker(\"브러시 색상 16진수 입력: \")\n",
        "bg_color = st.sidebar.color_picker(\"배경색 16진수 입력: \", \"#FFFFFF\")\n",
        "\n",
        "# Create a canvas component\n",
        "canvas = st_canvas(\n",
        "    stroke_width=b_width,\n",
        "    stroke_color=b_color,\n",
        "    background_color=bg_color,\n",
        "    update_streamlit=True,\n",
        "    height=300,\n",
        "    width=300,\n",
        "    drawing_mode='freedraw',\n",
        "    key=\"canvas\",\n",
        ")\n",
        "\n",
        "if canvas.image_data is not None:\n",
        "    image = Image.fromarray(canvas.image_data)\n",
        "    w, h = image.size\n",
        "    st.image(image, width=500, caption=\"숫자 이미지\")\n",
        "    import_and_predict(image, model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
