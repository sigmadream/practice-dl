{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Day7. LSTM 학습하기 실습(Hands-On)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iWIw3d9hofA"
      },
      "source": [
        "## 데이터 살펴보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUYHv3QyXBq5",
        "outputId": "c99cda15-38d4-494b-c487-e7b7fa97232a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./data/LSTM/ArticlesApril2017.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6Vd-BH4h2AK"
      },
      "source": [
        "## 학습용 데이터셋 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1QtO1ZfsXmJh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import string\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "class TextGeneration(Dataset):\n",
        "    def __init__(self):\n",
        "        all_headlines = []\n",
        "        # 모든 헤드라인의 텍스트를 불러옴\n",
        "        for filename in glob.glob(\"./data/LSTM/*.csv\"):\n",
        "            if 'Articles' in filename:\n",
        "                article_df = pd.read_csv(filename)\n",
        "                all_headlines.extend(list(article_df.headline.values))\n",
        "                break\n",
        "\n",
        "        # headline 중 unknown 값은 제거\n",
        "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "\n",
        "        # 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
        "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
        "        self.BOW = {}\n",
        "\n",
        "        # 모든 문장의 단어를 추출해 고유번호 지정\n",
        "        for line in self.corpus:\n",
        "            for word in line.split():\n",
        "                if word not in self.BOW.keys():\n",
        "                    self.BOW[word] = len(self.BOW.keys())\n",
        "\n",
        "        # 모델의 입력으로 사용할 데이터\n",
        "        self.data = self.generate_sequence(self.corpus)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        data = np.array(self.data[i][0]) # 입력 데이터\n",
        "        label = np.array(self.data[i][1]).astype(np.float32) # 출력 데이터\n",
        "        return data, label\n",
        "\n",
        "    def clean_text(self, txt):\n",
        "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
        "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "        return txt\n",
        "\n",
        "    def generate_sequence(self, txt):\n",
        "        seq = []\n",
        "\n",
        "        for line in txt:\n",
        "            line = line.split()\n",
        "            line_bow = [self.BOW[word] for word in line]\n",
        "\n",
        "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
        "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2])\n",
        "            for i in range(len(line_bow)-2)]\n",
        "\n",
        "            seq.extend(data)\n",
        "\n",
        "        return seq\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx0YMUARiK9E"
      },
      "source": [
        "## LSTM모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iQ_NQkA7llFJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "   def __init__(self, num_embeddings):\n",
        "       super(LSTM, self).__init__()\n",
        "\n",
        "       # 밀집표현을 위한 임베딩층\n",
        "       self.embed = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=16)\n",
        "\n",
        "       # LSTM을 5개층을 쌓음\n",
        "       self.lstm = nn.LSTM(\n",
        "           input_size=16,\n",
        "           hidden_size=64,\n",
        "           num_layers=5,\n",
        "           batch_first=True)\n",
        "\n",
        "       # 분류를 위한 MLP층\n",
        "       self.fc1 = nn.Linear(128, num_embeddings)\n",
        "       self.fc2 = nn.Linear(num_embeddings,num_embeddings)\n",
        "\n",
        "       # 활성화 함수\n",
        "       self.relu = nn.ReLU()\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = self.embed(x)\n",
        "\n",
        "       # LSTM 모델의 예측값\n",
        "       x, _ = self.lstm(x)\n",
        "       x = torch.reshape(x, (x.shape[0], -1))\n",
        "       x = self.fc1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.fc2(x)\n",
        "\n",
        "       return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHbQlFoZiRN7"
      },
      "source": [
        "## 모델 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E9x_OCUjnGe5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/63 [00:00<?, ?it/s]C:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_14424\\954448527.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
            "C:\\Users\\sigma\\AppData\\Local\\Temp\\ipykernel_14424\\954448527.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred, torch.tensor(label, dtype=torch.long).to(device))\n",
            "epoch0 loss:7.372651100158691: 100%|██████████| 63/63 [00:02<00:00, 29.96it/s] \n",
            "epoch1 loss:7.012795448303223: 100%|██████████| 63/63 [00:02<00:00, 30.09it/s] \n",
            "epoch2 loss:6.751251220703125: 100%|██████████| 63/63 [00:02<00:00, 30.34it/s] \n",
            "epoch3 loss:6.5387091636657715: 100%|██████████| 63/63 [00:02<00:00, 30.41it/s]\n",
            "epoch4 loss:6.4103546142578125: 100%|██████████| 63/63 [00:02<00:00, 30.51it/s]\n",
            "epoch5 loss:6.107255935668945: 100%|██████████| 63/63 [00:02<00:00, 30.87it/s] \n",
            "epoch6 loss:5.7916669845581055: 100%|██████████| 63/63 [00:02<00:00, 30.93it/s]\n",
            "epoch7 loss:5.595593452453613: 100%|██████████| 63/63 [00:02<00:00, 31.11it/s] \n",
            "epoch8 loss:5.431673049926758: 100%|██████████| 63/63 [00:01<00:00, 31.54it/s] \n",
            "epoch9 loss:5.613152980804443: 100%|██████████| 63/63 [00:02<00:00, 31.12it/s] \n",
            "epoch10 loss:5.555459022521973: 100%|██████████| 63/63 [00:02<00:00, 31.03it/s] \n",
            "epoch11 loss:5.417506217956543: 100%|██████████| 63/63 [00:02<00:00, 30.25it/s] \n",
            "epoch12 loss:5.819920063018799: 100%|██████████| 63/63 [00:02<00:00, 30.95it/s] \n",
            "epoch13 loss:5.562602996826172: 100%|██████████| 63/63 [00:02<00:00, 30.69it/s] \n",
            "epoch14 loss:5.339858055114746: 100%|██████████| 63/63 [00:02<00:00, 31.18it/s] \n",
            "epoch15 loss:5.221261501312256: 100%|██████████| 63/63 [00:02<00:00, 31.21it/s] \n",
            "epoch16 loss:5.118991851806641: 100%|██████████| 63/63 [00:02<00:00, 30.98it/s] \n",
            "epoch17 loss:4.925679683685303: 100%|██████████| 63/63 [00:02<00:00, 31.03it/s] \n",
            "epoch18 loss:4.803688049316406: 100%|██████████| 63/63 [00:02<00:00, 30.64it/s] \n",
            "epoch19 loss:4.697256088256836: 100%|██████████| 63/63 [00:02<00:00, 30.57it/s] \n",
            "epoch20 loss:4.366311073303223: 100%|██████████| 63/63 [00:02<00:00, 30.25it/s] \n",
            "epoch21 loss:4.232382774353027: 100%|██████████| 63/63 [00:02<00:00, 30.47it/s] \n",
            "epoch22 loss:4.235006332397461: 100%|██████████| 63/63 [00:02<00:00, 30.69it/s] \n",
            "epoch23 loss:4.18096399307251: 100%|██████████| 63/63 [00:02<00:00, 30.41it/s]  \n",
            "epoch24 loss:4.231608867645264: 100%|██████████| 63/63 [00:02<00:00, 30.50it/s] \n",
            "epoch25 loss:4.260599613189697: 100%|██████████| 63/63 [00:02<00:00, 30.30it/s] \n",
            "epoch26 loss:4.290002822875977: 100%|██████████| 63/63 [00:02<00:00, 30.28it/s] \n",
            "epoch27 loss:4.587839603424072: 100%|██████████| 63/63 [00:02<00:00, 30.50it/s] \n",
            "epoch28 loss:4.5711822509765625: 100%|██████████| 63/63 [00:02<00:00, 30.52it/s]\n",
            "epoch29 loss:4.351805686950684: 100%|██████████| 63/63 [00:02<00:00, 30.46it/s] \n",
            "epoch30 loss:4.152091979980469: 100%|██████████| 63/63 [00:02<00:00, 30.27it/s] \n",
            "epoch31 loss:3.99059796333313: 100%|██████████| 63/63 [00:02<00:00, 30.00it/s]  \n",
            "epoch32 loss:3.88297963142395: 100%|██████████| 63/63 [00:02<00:00, 30.40it/s]  \n",
            "epoch33 loss:3.866213321685791: 100%|██████████| 63/63 [00:02<00:00, 30.43it/s] \n",
            "epoch34 loss:3.689802885055542: 100%|██████████| 63/63 [00:02<00:00, 30.59it/s] \n",
            "epoch35 loss:3.6054000854492188: 100%|██████████| 63/63 [00:02<00:00, 30.76it/s]\n",
            "epoch36 loss:3.6056625843048096: 100%|██████████| 63/63 [00:02<00:00, 30.59it/s]\n",
            "epoch37 loss:3.5783417224884033: 100%|██████████| 63/63 [00:02<00:00, 30.81it/s]\n",
            "epoch38 loss:3.487673282623291: 100%|██████████| 63/63 [00:02<00:00, 30.68it/s] \n",
            "epoch39 loss:3.4938406944274902: 100%|██████████| 63/63 [00:02<00:00, 30.92it/s]\n",
            "epoch40 loss:3.420072555541992: 100%|██████████| 63/63 [00:02<00:00, 30.37it/s] \n",
            "epoch41 loss:3.252845048904419: 100%|██████████| 63/63 [00:02<00:00, 30.46it/s] \n",
            "epoch42 loss:3.2274937629699707: 100%|██████████| 63/63 [00:02<00:00, 30.38it/s]\n",
            "epoch43 loss:3.2877650260925293: 100%|██████████| 63/63 [00:02<00:00, 30.76it/s]\n",
            "epoch44 loss:3.256868362426758: 100%|██████████| 63/63 [00:02<00:00, 30.70it/s] \n",
            "epoch45 loss:3.279987335205078: 100%|██████████| 63/63 [00:02<00:00, 30.61it/s] \n",
            "epoch46 loss:3.1522459983825684: 100%|██████████| 63/63 [00:02<00:00, 30.49it/s]\n",
            "epoch47 loss:3.078834056854248: 100%|██████████| 63/63 [00:02<00:00, 29.68it/s] \n",
            "epoch48 loss:3.0076518058776855: 100%|██████████| 63/63 [00:02<00:00, 30.35it/s]\n",
            "epoch49 loss:3.0695154666900635: 100%|██████████| 63/63 [00:02<00:00, 30.25it/s]\n",
            "epoch50 loss:3.0221283435821533: 100%|██████████| 63/63 [00:02<00:00, 30.30it/s]\n",
            "epoch51 loss:3.1195931434631348: 100%|██████████| 63/63 [00:02<00:00, 28.72it/s]\n",
            "epoch52 loss:3.0998075008392334: 100%|██████████| 63/63 [00:02<00:00, 30.08it/s]\n",
            "epoch53 loss:3.062981367111206: 100%|██████████| 63/63 [00:02<00:00, 29.97it/s] \n",
            "epoch54 loss:3.137132167816162: 100%|██████████| 63/63 [00:02<00:00, 30.58it/s] \n",
            "epoch55 loss:3.2123146057128906: 100%|██████████| 63/63 [00:02<00:00, 30.41it/s]\n",
            "epoch56 loss:3.0057947635650635: 100%|██████████| 63/63 [00:02<00:00, 29.94it/s]\n",
            "epoch57 loss:2.5904428958892822: 100%|██████████| 63/63 [00:02<00:00, 29.25it/s]\n",
            "epoch58 loss:2.33766770362854: 100%|██████████| 63/63 [00:02<00:00, 29.82it/s]  \n",
            "epoch59 loss:2.209383010864258: 100%|██████████| 63/63 [00:02<00:00, 29.93it/s] \n",
            "epoch60 loss:2.1444334983825684: 100%|██████████| 63/63 [00:02<00:00, 30.01it/s]\n",
            "epoch61 loss:2.1039631366729736: 100%|██████████| 63/63 [00:02<00:00, 30.02it/s]\n",
            "epoch62 loss:2.2292325496673584: 100%|██████████| 63/63 [00:02<00:00, 29.69it/s]\n",
            "epoch63 loss:2.423673152923584: 100%|██████████| 63/63 [00:02<00:00, 29.88it/s] \n",
            "epoch64 loss:2.3762269020080566: 100%|██████████| 63/63 [00:02<00:00, 29.70it/s]\n",
            "epoch65 loss:2.256706476211548: 100%|██████████| 63/63 [00:02<00:00, 29.23it/s] \n",
            "epoch66 loss:2.3110923767089844: 100%|██████████| 63/63 [00:02<00:00, 29.09it/s]\n",
            "epoch67 loss:2.196319580078125: 100%|██████████| 63/63 [00:02<00:00, 29.65it/s] \n",
            "epoch68 loss:1.9813998937606812: 100%|██████████| 63/63 [00:02<00:00, 29.74it/s]\n",
            "epoch69 loss:1.7943894863128662: 100%|██████████| 63/63 [00:02<00:00, 29.72it/s]\n",
            "epoch70 loss:1.5957521200180054: 100%|██████████| 63/63 [00:02<00:00, 29.39it/s]\n",
            "epoch71 loss:1.5631855726242065: 100%|██████████| 63/63 [00:02<00:00, 29.32it/s]\n",
            "epoch72 loss:1.4813250303268433: 100%|██████████| 63/63 [00:02<00:00, 29.99it/s]\n",
            "epoch73 loss:1.4767897129058838: 100%|██████████| 63/63 [00:02<00:00, 29.92it/s]\n",
            "epoch74 loss:1.815490484237671: 100%|██████████| 63/63 [00:02<00:00, 29.78it/s] \n",
            "epoch75 loss:1.7830092906951904: 100%|██████████| 63/63 [00:02<00:00, 29.06it/s]\n",
            "epoch76 loss:1.8959804773330688: 100%|██████████| 63/63 [00:02<00:00, 29.32it/s]\n",
            "epoch77 loss:1.5385154485702515: 100%|██████████| 63/63 [00:02<00:00, 30.23it/s]\n",
            "epoch78 loss:1.5313900709152222: 100%|██████████| 63/63 [00:02<00:00, 29.57it/s]\n",
            "epoch79 loss:2.7489333152770996: 100%|██████████| 63/63 [00:02<00:00, 29.86it/s]\n",
            "epoch80 loss:1.4279186725616455: 100%|██████████| 63/63 [00:02<00:00, 29.64it/s]\n",
            "epoch81 loss:1.2948559522628784: 100%|██████████| 63/63 [00:02<00:00, 29.60it/s]\n",
            "epoch82 loss:1.1474590301513672: 100%|██████████| 63/63 [00:02<00:00, 29.71it/s]\n",
            "epoch83 loss:1.130650281906128: 100%|██████████| 63/63 [00:02<00:00, 29.80it/s] \n",
            "epoch84 loss:1.0365859270095825: 100%|██████████| 63/63 [00:02<00:00, 29.07it/s]\n",
            "epoch85 loss:1.150331735610962: 100%|██████████| 63/63 [00:02<00:00, 29.89it/s] \n",
            "epoch86 loss:0.9882837533950806: 100%|██████████| 63/63 [00:02<00:00, 29.60it/s]\n",
            "epoch87 loss:1.1159240007400513: 100%|██████████| 63/63 [00:02<00:00, 29.68it/s]\n",
            "epoch88 loss:0.7778836488723755: 100%|██████████| 63/63 [00:02<00:00, 29.50it/s]\n",
            "epoch89 loss:0.8133600950241089: 100%|██████████| 63/63 [00:02<00:00, 29.56it/s]\n",
            "epoch90 loss:0.8302339315414429: 100%|██████████| 63/63 [00:02<00:00, 29.79it/s]\n",
            "epoch91 loss:0.8861123323440552: 100%|██████████| 63/63 [00:02<00:00, 28.76it/s]\n",
            "epoch92 loss:0.8457903861999512: 100%|██████████| 63/63 [00:02<00:00, 29.90it/s]\n",
            "epoch93 loss:0.795343816280365: 100%|██████████| 63/63 [00:02<00:00, 28.97it/s] \n",
            "epoch94 loss:0.8793076276779175: 100%|██████████| 63/63 [00:02<00:00, 28.90it/s]\n",
            "epoch95 loss:0.9502432942390442: 100%|██████████| 63/63 [00:02<00:00, 29.64it/s]\n",
            "epoch96 loss:1.0178048610687256: 100%|██████████| 63/63 [00:02<00:00, 28.79it/s]\n",
            "epoch97 loss:0.9928147196769714: 100%|██████████| 63/63 [00:02<00:00, 29.41it/s]\n",
            "epoch98 loss:0.9010856747627258: 100%|██████████| 63/63 [00:02<00:00, 28.45it/s]\n",
            "epoch99 loss:0.692245364189148: 100%|██████████| 63/63 [00:02<00:00, 29.58it/s] \n",
            "epoch100 loss:0.6776093244552612: 100%|██████████| 63/63 [00:02<00:00, 29.61it/s]\n",
            "epoch101 loss:0.6116495132446289: 100%|██████████| 63/63 [00:02<00:00, 29.68it/s]\n",
            "epoch102 loss:0.6006203293800354: 100%|██████████| 63/63 [00:02<00:00, 29.55it/s]\n",
            "epoch103 loss:0.7315515279769897: 100%|██████████| 63/63 [00:02<00:00, 29.77it/s]\n",
            "epoch104 loss:0.6399082541465759: 100%|██████████| 63/63 [00:02<00:00, 29.57it/s]\n",
            "epoch105 loss:0.6313225626945496: 100%|██████████| 63/63 [00:02<00:00, 29.78it/s]\n",
            "epoch106 loss:0.5401725172996521: 100%|██████████| 63/63 [00:02<00:00, 29.48it/s]\n",
            "epoch107 loss:0.6542189717292786: 100%|██████████| 63/63 [00:02<00:00, 29.56it/s]\n",
            "epoch108 loss:1.0974290370941162: 100%|██████████| 63/63 [00:02<00:00, 29.60it/s]\n",
            "epoch109 loss:0.5562237501144409: 100%|██████████| 63/63 [00:02<00:00, 29.08it/s]\n",
            "epoch110 loss:0.7701835632324219: 100%|██████████| 63/63 [00:02<00:00, 29.05it/s]\n",
            "epoch111 loss:0.7525618076324463: 100%|██████████| 63/63 [00:02<00:00, 29.39it/s]\n",
            "epoch112 loss:0.5408453345298767: 100%|██████████| 63/63 [00:02<00:00, 29.75it/s]\n",
            "epoch113 loss:0.5820992588996887: 100%|██████████| 63/63 [00:02<00:00, 28.39it/s]\n",
            "epoch114 loss:0.43132632970809937: 100%|██████████| 63/63 [00:02<00:00, 28.90it/s]\n",
            "epoch115 loss:0.4582231938838959: 100%|██████████| 63/63 [00:02<00:00, 29.07it/s]\n",
            "epoch116 loss:0.46172642707824707: 100%|██████████| 63/63 [00:02<00:00, 28.57it/s]\n",
            "epoch117 loss:0.4117851257324219: 100%|██████████| 63/63 [00:02<00:00, 28.44it/s]\n",
            "epoch118 loss:0.4054495692253113: 100%|██████████| 63/63 [00:02<00:00, 28.75it/s]\n",
            "epoch119 loss:0.4143424928188324: 100%|██████████| 63/63 [00:02<00:00, 28.26it/s]\n",
            "epoch120 loss:0.399949312210083: 100%|██████████| 63/63 [00:02<00:00, 28.13it/s] \n",
            "epoch121 loss:0.2994658350944519: 100%|██████████| 63/63 [00:02<00:00, 28.44it/s]\n",
            "epoch122 loss:0.31716188788414: 100%|██████████| 63/63 [00:02<00:00, 27.85it/s]  \n",
            "epoch123 loss:0.2917465269565582: 100%|██████████| 63/63 [00:02<00:00, 27.43it/s]\n",
            "epoch124 loss:0.30665263533592224: 100%|██████████| 63/63 [00:02<00:00, 27.29it/s]\n",
            "epoch125 loss:0.30644601583480835: 100%|██████████| 63/63 [00:02<00:00, 26.71it/s]\n",
            "epoch126 loss:0.3023678660392761: 100%|██████████| 63/63 [00:02<00:00, 28.01it/s]\n",
            "epoch127 loss:0.3063408136367798: 100%|██████████| 63/63 [00:02<00:00, 27.55it/s]\n",
            "epoch128 loss:0.36595290899276733: 100%|██████████| 63/63 [00:02<00:00, 27.07it/s]\n",
            "epoch129 loss:0.23219983279705048: 100%|██████████| 63/63 [00:02<00:00, 26.70it/s]\n",
            "epoch130 loss:0.8885325193405151: 100%|██████████| 63/63 [00:02<00:00, 26.96it/s]\n",
            "epoch131 loss:0.8072288632392883: 100%|██████████| 63/63 [00:02<00:00, 28.26it/s]\n",
            "epoch132 loss:0.7318798303604126: 100%|██████████| 63/63 [00:02<00:00, 28.36it/s]\n",
            "epoch133 loss:0.2506997883319855: 100%|██████████| 63/63 [00:02<00:00, 28.57it/s]\n",
            "epoch134 loss:0.9059635400772095: 100%|██████████| 63/63 [00:02<00:00, 28.60it/s]\n",
            "epoch135 loss:0.5220997929573059: 100%|██████████| 63/63 [00:02<00:00, 28.99it/s]\n",
            "epoch136 loss:0.3638765513896942: 100%|██████████| 63/63 [00:02<00:00, 28.91it/s]\n",
            "epoch137 loss:0.28650176525115967: 100%|██████████| 63/63 [00:02<00:00, 29.15it/s]\n",
            "epoch138 loss:0.23368768393993378: 100%|██████████| 63/63 [00:02<00:00, 28.99it/s]\n",
            "epoch139 loss:0.3135228753089905: 100%|██████████| 63/63 [00:02<00:00, 28.62it/s]\n",
            "epoch140 loss:0.29951217770576477: 100%|██████████| 63/63 [00:02<00:00, 28.51it/s]\n",
            "epoch141 loss:0.313153475522995: 100%|██████████| 63/63 [00:02<00:00, 28.38it/s] \n",
            "epoch142 loss:0.22673535346984863: 100%|██████████| 63/63 [00:02<00:00, 28.48it/s]\n",
            "epoch143 loss:0.3167438507080078: 100%|██████████| 63/63 [00:02<00:00, 28.56it/s]\n",
            "epoch144 loss:0.23271474242210388: 100%|██████████| 63/63 [00:02<00:00, 27.62it/s]\n",
            "epoch145 loss:0.2790517210960388: 100%|██████████| 63/63 [00:02<00:00, 28.10it/s]\n",
            "epoch146 loss:0.15404677391052246: 100%|██████████| 63/63 [00:02<00:00, 28.23it/s]\n",
            "epoch147 loss:0.19027599692344666: 100%|██████████| 63/63 [00:02<00:00, 26.29it/s]\n",
            "epoch148 loss:0.19010695815086365: 100%|██████████| 63/63 [00:02<00:00, 28.20it/s]\n",
            "epoch149 loss:0.20009469985961914: 100%|██████████| 63/63 [00:02<00:00, 27.51it/s]\n",
            "epoch150 loss:0.3250100314617157: 100%|██████████| 63/63 [00:02<00:00, 27.48it/s]\n",
            "epoch151 loss:0.2491767853498459: 100%|██████████| 63/63 [00:02<00:00, 26.99it/s] \n",
            "epoch152 loss:0.36567503213882446: 100%|██████████| 63/63 [00:02<00:00, 27.58it/s]\n",
            "epoch153 loss:0.19784851372241974: 100%|██████████| 63/63 [00:02<00:00, 27.47it/s]\n",
            "epoch154 loss:0.2207948863506317: 100%|██████████| 63/63 [00:02<00:00, 27.28it/s]\n",
            "epoch155 loss:0.20343296229839325: 100%|██████████| 63/63 [00:02<00:00, 27.01it/s]\n",
            "epoch156 loss:0.14597663283348083: 100%|██████████| 63/63 [00:02<00:00, 27.80it/s]\n",
            "epoch157 loss:0.15305744111537933: 100%|██████████| 63/63 [00:02<00:00, 27.74it/s]\n",
            "epoch158 loss:0.192966490983963: 100%|██████████| 63/63 [00:02<00:00, 27.63it/s]  \n",
            "epoch159 loss:0.20738768577575684: 100%|██████████| 63/63 [00:02<00:00, 27.26it/s]\n",
            "epoch160 loss:0.17042401432991028: 100%|██████████| 63/63 [00:02<00:00, 26.77it/s]\n",
            "epoch161 loss:0.14671185612678528: 100%|██████████| 63/63 [00:02<00:00, 27.14it/s]\n",
            "epoch162 loss:0.2155202180147171: 100%|██████████| 63/63 [00:02<00:00, 27.28it/s] \n",
            "epoch163 loss:0.19189725816249847: 100%|██████████| 63/63 [00:02<00:00, 26.13it/s]\n",
            "epoch164 loss:0.21228453516960144: 100%|██████████| 63/63 [00:02<00:00, 26.85it/s]\n",
            "epoch165 loss:0.21198499202728271: 100%|██████████| 63/63 [00:02<00:00, 27.14it/s]\n",
            "epoch166 loss:0.1336313784122467: 100%|██████████| 63/63 [00:02<00:00, 27.05it/s] \n",
            "epoch167 loss:0.10597109794616699: 100%|██████████| 63/63 [00:02<00:00, 26.92it/s]\n",
            "epoch168 loss:0.31624317169189453: 100%|██████████| 63/63 [00:02<00:00, 27.22it/s]\n",
            "epoch169 loss:0.1102648377418518: 100%|██████████| 63/63 [00:02<00:00, 27.46it/s]\n",
            "epoch170 loss:0.1352931559085846: 100%|██████████| 63/63 [00:02<00:00, 27.52it/s] \n",
            "epoch171 loss:0.2176336944103241: 100%|██████████| 63/63 [00:02<00:00, 27.30it/s] \n",
            "epoch172 loss:0.17053921520709991: 100%|██████████| 63/63 [00:02<00:00, 27.28it/s]\n",
            "epoch173 loss:0.12344326823949814: 100%|██████████| 63/63 [00:02<00:00, 27.85it/s]\n",
            "epoch174 loss:0.2963258624076843: 100%|██████████| 63/63 [00:02<00:00, 28.16it/s]\n",
            "epoch175 loss:0.2578527331352234: 100%|██████████| 63/63 [00:02<00:00, 27.97it/s] \n",
            "epoch176 loss:0.17342731356620789: 100%|██████████| 63/63 [00:02<00:00, 27.88it/s]\n",
            "epoch177 loss:0.18770797550678253: 100%|██████████| 63/63 [00:02<00:00, 25.92it/s]\n",
            "epoch178 loss:0.2669675052165985: 100%|██████████| 63/63 [00:02<00:00, 28.11it/s] \n",
            "epoch179 loss:0.23345589637756348: 100%|██████████| 63/63 [00:02<00:00, 27.99it/s]\n",
            "epoch180 loss:0.18830612301826477: 100%|██████████| 63/63 [00:02<00:00, 27.36it/s]\n",
            "epoch181 loss:0.1043897271156311: 100%|██████████| 63/63 [00:02<00:00, 27.13it/s] \n",
            "epoch182 loss:0.09840714931488037: 100%|██████████| 63/63 [00:02<00:00, 27.77it/s]\n",
            "epoch183 loss:0.08853331953287125: 100%|██████████| 63/63 [00:02<00:00, 27.73it/s]\n",
            "epoch184 loss:0.13405174016952515: 100%|██████████| 63/63 [00:02<00:00, 27.43it/s]\n",
            "epoch185 loss:0.15336914360523224: 100%|██████████| 63/63 [00:02<00:00, 27.55it/s]\n",
            "epoch186 loss:0.09544318169355392: 100%|██████████| 63/63 [00:02<00:00, 27.75it/s]\n",
            "epoch187 loss:0.14581386744976044: 100%|██████████| 63/63 [00:02<00:00, 27.17it/s]\n",
            "epoch188 loss:0.16919568181037903: 100%|██████████| 63/63 [00:02<00:00, 27.42it/s]\n",
            "epoch189 loss:0.11675361543893814: 100%|██████████| 63/63 [00:02<00:00, 27.53it/s]\n",
            "epoch190 loss:0.21376530826091766: 100%|██████████| 63/63 [00:02<00:00, 27.66it/s]\n",
            "epoch191 loss:0.19200748205184937: 100%|██████████| 63/63 [00:02<00:00, 27.68it/s]\n",
            "epoch192 loss:0.1597258746623993: 100%|██████████| 63/63 [00:02<00:00, 27.99it/s] \n",
            "epoch193 loss:0.17169798910617828: 100%|██████████| 63/63 [00:02<00:00, 27.55it/s]\n",
            "epoch194 loss:0.37269127368927: 100%|██████████| 63/63 [00:02<00:00, 26.88it/s]   \n",
            "epoch195 loss:0.1793302744626999: 100%|██████████| 63/63 [00:02<00:00, 27.01it/s] \n",
            "epoch196 loss:0.22224219143390656: 100%|██████████| 63/63 [00:02<00:00, 27.37it/s]\n",
            "epoch197 loss:0.15241554379463196: 100%|██████████| 63/63 [00:02<00:00, 27.42it/s]\n",
            "epoch198 loss:0.14123687148094177: 100%|██████████| 63/63 [00:02<00:00, 27.67it/s]\n",
            "epoch199 loss:0.30991119146347046: 100%|██████████| 63/63 [00:02<00:00, 28.17it/s]\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "# 학습을 진행할 프로세서 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "dataset = TextGeneration()  # 데이터셋 정의\n",
        "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)  # 모델 정의\n",
        "loader = DataLoader(dataset, batch_size=64)\n",
        "optim = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(200):\n",
        "   iterator = tqdm.tqdm(loader)\n",
        "   for data, label in iterator:\n",
        "       # 기울기 초기화\n",
        "       optim.zero_grad()\n",
        "\n",
        "       # 모델의 예측값\n",
        "       pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
        "\n",
        "       # 정답 레이블은 long 텐서로 반환해야 함\n",
        "       loss = nn.CrossEntropyLoss()(\n",
        "           pred, torch.tensor(label, dtype=torch.long).to(device))\n",
        "\n",
        "       # 오차 역전파\n",
        "       loss.backward()\n",
        "       optim.step()\n",
        "\n",
        "       iterator.set_description(f\"epoch{epoch} loss:{loss.item()}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"lstm.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2aJFOvbn3Jm",
        "outputId": "f0bdce15-46fb-4a24-b2ba-06803cb1c1d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input word: finding an \n",
            "predicted sentence: finding an expansive view and dims it today today sister sharp in \n"
          ]
        }
      ],
      "source": [
        "def generate(model, BOW, string=\"finding an \", strlen=10):\n",
        "   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "   print(f\"input word: {string}\")\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for p in range(strlen):\n",
        "           # 입력 문장을 텐서로 변경\n",
        "           words = torch.tensor([BOW[w] for w in string.split()], dtype=torch.long).to(device)           \n",
        "           input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
        "           output = model(input_tensor)  # 모델을 이용해 예측\n",
        "           output_word = (torch.argmax(output).cpu().numpy())\n",
        "           string += list(BOW.keys())[output_word]  # 문장에 예측된 단어를 추가\n",
        "           string += \" \"\n",
        "\n",
        "   print(f\"predicted sentence: {string}\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"lstm.pth\", map_location=device))\n",
        "pred = generate(model, dataset.BOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28rPwg8SxImr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
