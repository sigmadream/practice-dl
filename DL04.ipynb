{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Day4. CNN을 사용한 특징 기반 분류 v1.2 실습(Hands-On)\n",
        "---\n",
        "\n",
        "> [CNN](https://hal.science/hal-03926082/document)을 학습하고, [VGG](https://arxiv.org/abs/1409.1556)에 대해서 알아보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- LeNet 이전의 분류 작업\n",
        "    - `Pattern`을 `수식적`으로 작성 => 분류기에 해당 수식을 사용해서 분류를 진행\n",
        "        - 예를 들어, 1) 입력 이미지가 주어지면 해당 입력에 대해 수학적인 방식으로 입력이미지의 특징들을 추출(면,선,코너 등의 정보), 2) 해당 특징들을 기반으로 적절한 특징들을 선별해서 해당 특징들의 벡터와 같은 추가적인 정보를 획득, 3) 최종적으로 이러한 정보들을 활용해 사물을 인식하는 과정을 거침\n",
        "    - 한계점\n",
        "        - 특징 추출에 많은 사전지식이 포함되어 있어야 함\n",
        "        - 특징 추출 과정에서 이러한 특징들을 어떻게 특징을 추출하는지에 따라 성능이 좌우\n",
        "        - 새로운 문제(새로운 특징이 추가적으로 필요하거나 기존과는 다른 패턴이 보이는 경우)에는 다시 시스템을 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- CNN(convolution neural network)\n",
        "    - CNN은 작은 필터를 이용해 이미지로부터 특징을 추출해내는 방법\n",
        "        - 기존 모델은 모든 픽셀에 대해 가중치를 갖고 있고, 전체를 특징으로 이용하기 때문에 학습에 사용된 데이터에 관해서는 완벽에 가까울 정도로 특징을 잡아낼 수 있는데, 특징 위치가 바뀌게 되면 무용지물\n",
        "        - 반대로 합성곱은 커널을 이미지 안에서 이리저리 움직이며 특징을 추출, 따라서 볼 수 있는 시야는 좁아지는 대신 위치와 무관하게 특징을 잡아 낼 수 있음\n",
        "        - 이미지 전체에 가중치를 두는 모델은 이미지 크기가 커지면 학습해야 하는 가중치 개수도 늘어나는데, CNN 커널 크기는 변화가 없음, 즉 커널을 사용하면 이미지 크기와 무관하게 학습해야 하는 가중치 개수가 같음\n",
        "        - 학습할 가중치도 줄어들고, 특징의 위치에 대해 어느 정도 자유로워졌으니 두 마리 토끼를 다 잡은 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [`LeNet-5`](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n",
        "    - `convolution neural network`를 활용\n",
        "    - CNN은 기존의 방식과는 다르게 이미지를 일정한 지역적인 패턴을 이용하는 특수한 구조\n",
        "    - 다중 신경망 네트워크는 복잡하고, 고차원적이고, 비선형 mapping을 이미지 인식 작업에서 여러 클래스를 맞추는 작업을 통해 오차를 계산해 Gradient Descent 방법을 통해 학습\n",
        "    - LeNet-5 이전의 패턴 인식 구조에서 Feature Extraction 작업은 CNN 구조를 통해 대체될 수 있고\n",
        "    - Classifier Module은 Fully connected layer를 통해 대체될 수 있음\n",
        "    - 이때 CNN과 Fully Connected layer이 가능하게 된 배경은 빠른 알고리즘과 적은 비용으로도 높은 성능의 컴퓨터를 사용할 수 있게 되면서 연산량이 많은 CNN과 Fully Connected layer를 사용할 수 있게 되었음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "transforms = Compose([\n",
        "   ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = torchvision.datasets.MNIST('data', train=True, download=True, transform=transforms)\n",
        "test = torchvision.datasets.MNIST('data', train=False, download=True, transform=transforms)\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=100, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### M.L.P(Baseline Model with Multilayer Perceptrons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(784, 784)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(784, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.act1(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: model accuracy 84.93%\n",
            "Epoch 1: model accuracy 88.31%\n",
            "Epoch 2: model accuracy 89.64%\n",
            "Epoch 3: model accuracy 90.02%\n",
            "Epoch 4: model accuracy 90.67%\n"
          ]
        }
      ],
      "source": [
        "model = MLP()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        y_pred = model(X_batch)\n",
        "        acc += (torch.argmax(y_pred, 1) == y_batch).float().sum()\n",
        "        count += len(y_batch)\n",
        "    acc = acc / count\n",
        "    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple Convolutional Neural Network for MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 합성곱\n",
        "- 커널 : 이미지로부터 특징을 추출하기 위한 가중치 행렬\n",
        "- 특징 : 합성곱의 결과로부터 얻어지는 이미지\n",
        "- stride : 커널이 움직이는 거리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(1, 10, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc = nn.Linear(27*27*10, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.output = nn.Linear(128, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu2(self.fc(self.flat(x)))\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: model accuracy 97.43%\n",
            "Epoch 1: model accuracy 98.01%\n",
            "Epoch 2: model accuracy 97.72%\n",
            "Epoch 3: model accuracy 98.06%\n",
            "Epoch 4: model accuracy 98.17%\n"
          ]
        }
      ],
      "source": [
        "model = CNN()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        y_pred = model(X_batch)\n",
        "        acc += (torch.argmax(y_pred, 1) == y_batch).float().sum()\n",
        "        count += len(y_batch)\n",
        "    acc = acc / count\n",
        "    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LeNet-5 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LeNet-5 for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0)\n",
        "        self.act3 = nn.Tanh()\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(1*1*120, 84)\n",
        "        self.act4 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.act1(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.act2(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.act3(self.conv3(x))\n",
        "        x = self.act4(self.fc1(self.flat(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: model accuracy 95.43%\n",
            "Epoch 1: model accuracy 97.16%\n",
            "Epoch 2: model accuracy 97.77%\n",
            "Epoch 3: model accuracy 97.76%\n",
            "Epoch 4: model accuracy 98.44%\n"
          ]
        }
      ],
      "source": [
        "model = LeNet5()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    count = 0\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        y_pred = model(X_batch)\n",
        "        acc += (torch.argmax(y_pred, 1) == y_batch).float().sum()\n",
        "        count += len(y_batch)\n",
        "    acc = acc / count\n",
        "    print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"LeNet5_20240129.pth\") # 모델 저장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN 모델 파라매터 확인 실습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "텐서의 크기 : torch.Size([1, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "# 임의의 텐서를 강제로 생성\n",
        "inputs = torch.Tensor(1, 1, 28, 28)\n",
        "print('텐서의 크기 : {}'.format(inputs.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n"
          ]
        }
      ],
      "source": [
        "conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)\n",
        "print(conv1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
          ]
        }
      ],
      "source": [
        "conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "print(conv2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
          ]
        }
      ],
      "source": [
        "pool = nn.MaxPool2d(2)\n",
        "print(pool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 32, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "out = conv1(inputs)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 32, 14, 14])\n"
          ]
        }
      ],
      "source": [
        "out = pool(out)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 64, 14, 14])\n"
          ]
        }
      ],
      "source": [
        "out = conv2(out)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 64, 7, 7])\n"
          ]
        }
      ],
      "source": [
        "out = pool(out)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 64 7 7\n"
          ]
        }
      ],
      "source": [
        "print(out.size(0), out.size(1), out.size(2), out.size(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3136])\n"
          ]
        }
      ],
      "source": [
        "# 첫번째 차원인 배치 차원은 그대로 두고 나머지는 펼쳐라\n",
        "out = out.view(out.size(0), -1) \n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "fc = nn.Linear(3136, 10) # input_dim = 3,136, output_dim = 10\n",
        "out = fc(out)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View와 Reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.arange(12)\n",
        "print(x) # tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10, 11]])\n",
            "tensor([[ 0,  1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10, 11]])\n"
          ]
        }
      ],
      "source": [
        "print(x.reshape(2, 6))\n",
        "print(x.view(2, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0,  1],\n",
            "         [ 2,  3],\n",
            "         [ 4,  5]],\n",
            "\n",
            "        [[ 6,  7],\n",
            "         [ 8,  9],\n",
            "         [10, 11]]])\n",
            "tensor([[[ 0,  1,  2],\n",
            "         [ 3,  4,  5]],\n",
            "\n",
            "        [[ 6,  7,  8],\n",
            "         [ 9, 10, 11]]])\n"
          ]
        }
      ],
      "source": [
        "print(x.reshape(2, 3, -1)) # (2, 3, 2) 차원으로 자동 지정\n",
        "print(x.view(2, 2, -1)) # (2, 2, 3) 차원으로 자동 지정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "torch.view와 torch.reshape의 가장 큰 차이는 contiguous 속성을 만족하지 않는 텐서에 적용이 가능하느냐 여부입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2699,  0.2880,  0.0943],\n",
            "        [ 0.2418,  1.3825,  0.3678],\n",
            "        [ 0.2878,  0.4216, -0.3238],\n",
            "        [ 0.8696,  0.7349, -0.2613]])\n",
            "tensor([[-0.0263, -2.1660, -0.6443],\n",
            "        [-0.5453, -1.3778, -1.1625],\n",
            "        [-0.9622, -0.5388,  0.1624],\n",
            "        [-0.0874, -0.0213,  0.4618]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.randn(3, 4)\n",
        "a.transpose_(0, 1)\n",
        "b = torch.randn(4, 3)\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a : 3044972495872\n",
            "a : 3044972495888\n",
            "a : 3044972495904\n",
            "a : 3044972495876\n",
            "a : 3044972495892\n",
            "a : 3044972495908\n",
            "a : 3044972495880\n",
            "a : 3044972495896\n",
            "a : 3044972495912\n",
            "a : 3044972495884\n",
            "a : 3044972495900\n",
            "a : 3044972495916\n",
            "b : 3044972496000\n",
            "b : 3044972496004\n",
            "b : 3044972496008\n",
            "b : 3044972496012\n",
            "b : 3044972496016\n",
            "b : 3044972496020\n",
            "b : 3044972496024\n",
            "b : 3044972496028\n",
            "b : 3044972496032\n",
            "b : 3044972496036\n",
            "b : 3044972496040\n",
            "b : 3044972496044\n"
          ]
        }
      ],
      "source": [
        "# a 텐서 메모리 주소 예시\n",
        "for i in range(4):\n",
        "    for j in range(3):\n",
        "        print(\"a :\", a[i][j].data_ptr())\n",
        "# b 텐서 메모리 주소 예시\n",
        "for i in range(4):\n",
        "    for j in range(3):\n",
        "        print(\"b :\", b[i][j].data_ptr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b는 axis = 0인 오른쪽 방향으로 자료가 순서대로 저장됨에 비해, a는 transpose 연산을 거치며 axis = 1인 아래 방향으로 자료가 저장되고 있었습니다. 여기서, b처럼 axis 순서대로 자료가 저장된 상태를 contiguous = True 상태라고 부르며, a같이 자료 저장 순서가 원래 방향과 어긋난 경우를 contiguous = False 상태라고 합니다.\n",
        "\n",
        "텐서의 shape을 조작하는 과정에서 메모리 저장 상태가 변경되는 경우가 있습니다. 주로 narrow(), view(), expand(), transpose() 등 메소드를 사용하는 경우에 이 상태가 깨지는 것으로 알려져 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = torch.ones(3, 4)\n",
        "y.transpose_(0, 1)\n",
        "y.is_contiguous() # False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[1., 1.],\n",
            "         [1., 1.]],\n",
            "\n",
            "        [[1., 1.],\n",
            "         [1., 1.]],\n",
            "\n",
            "        [[1., 1.],\n",
            "         [1., 1.]]])\n"
          ]
        }
      ],
      "source": [
        "print(y.reshape(3, 2, 2)) # 실행 가능\n",
        "# print(y.view(3, 2, 2)) # 실행 불가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
        "```\n",
        "\n",
        "따라서, 차원 변환을 적용하려는 텐서의 상태에 대하여 정확하게 파악하기가 모호한 경우에는 view 대신 reshape를 사용하시는 것을 권장드립니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "MLP                                      [1, 10]                   --\n",
              "├─Linear: 1-1                            [1, 784]                  615,440\n",
              "├─ReLU: 1-2                              [1, 784]                  --\n",
              "├─Linear: 1-3                            [1, 10]                   7,850\n",
              "==========================================================================================\n",
              "Total params: 623,290\n",
              "Trainable params: 623,290\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.62\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 2.49\n",
              "Estimated Total Size (MB): 2.50\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "summary(model, input_size=(1, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def findConv2dOutShape(H_in, W_in, conv, pool=2):\n",
        "    kernel_size = conv.kernel_size\n",
        "    stride = conv.stride\n",
        "    padding = conv.padding\n",
        "    dilation = conv.dilation\n",
        "\n",
        "    H_out = np.floor((H_in + 2*padding[0] - dilation[0]*(kernel_size[0]-1)-1) / stride[0] + 1)\n",
        "    W_out = np.floor((W_in + 2*padding[1] - dilation[1]*(kernel_size[1]-1)-1) / stride[1] + 1)\n",
        "\n",
        "    if pool:\n",
        "        H_out /= pool\n",
        "        W_out /= pool\n",
        "    \n",
        "    return int(H_out), int(W_out)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Net, self).__init__()\n",
        "        C_in, H_in, W_in = params['input_shape']\n",
        "        init_f = params['initial_filters']\n",
        "        num_fc1 = params['num_fc1']\n",
        "        num_classes = params['num_classes']\n",
        "        self.dropout_rate = params['dropout_rate']\n",
        "\n",
        "        self.conv1 = nn.Conv2d(C_in, init_f, kernel_size=3)\n",
        "        h,w = findConv2dOutShape(H_in, W_in, self.conv1)\n",
        "        self.conv2 = nn.Conv2d(init_f, 2*init_f, kernel_size=3)\n",
        "        h,w = findConv2dOutShape(h, w, self.conv2)\n",
        "        self.conv3 = nn.Conv2d(2*init_f, 4*init_f, kernel_size=3)\n",
        "        h,w = findConv2dOutShape(h, w, self.conv3)\n",
        "        self.conv4 = nn.Conv2d(4*init_f, 8*init_f, kernel_size=3)\n",
        "        h,w = findConv2dOutShape(h, w, self.conv4)\n",
        "\n",
        "        self.num_flatten = h*w*8*init_f\n",
        "        self.fc1 = nn.Linear(self.num_flatten, num_fc1)\n",
        "        self.fc2 = nn.Linear(num_fc1, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, self.num_flatten)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, self.dropout_rate, training = self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0035e0b4c8d24b2fa4ce11c618de1f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "418cefc9035345c98998cc4abe940915": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b32462f3056547d494ab178bd883fb3e",
            "placeholder": "​",
            "style": "IPY_MODEL_8daf6ae8edd54a27bc71a81930542a87",
            "value": ""
          }
        },
        "421ad834595a43cc97f7754d31662408": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87bf632f0e9742df8bf496b4b68101a0",
            "placeholder": "​",
            "style": "IPY_MODEL_0035e0b4c8d24b2fa4ce11c618de1f56",
            "value": " 170499072/? [00:11&lt;00:00, 16851595.48it/s]"
          }
        },
        "65fc1ef54933436f8a05dbcd3104ae11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76316f8307204b1bbf9960637bd7dc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_418cefc9035345c98998cc4abe940915",
              "IPY_MODEL_feb66dacf46a4934802bde29d489d574",
              "IPY_MODEL_421ad834595a43cc97f7754d31662408"
            ],
            "layout": "IPY_MODEL_ce9fe6e2279f45cc9ac93dddb5cc84b4"
          }
        },
        "87bf632f0e9742df8bf496b4b68101a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a5eddfa2fd14a6db77bdeeebbe57e99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8daf6ae8edd54a27bc71a81930542a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b32462f3056547d494ab178bd883fb3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce9fe6e2279f45cc9ac93dddb5cc84b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feb66dacf46a4934802bde29d489d574": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a5eddfa2fd14a6db77bdeeebbe57e99",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65fc1ef54933436f8a05dbcd3104ae11",
            "value": 170498071
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
